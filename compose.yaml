# compose.yaml
services:
  # --- PostgreSQL ---
  postgres:
    image: postgres:17
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: mlflowdb
    ports: ["5432:5432"]
    volumes: ["./postgres-data:/var/lib/postgresql/data"]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user -d mlflowdb"]
      interval: 5s
      timeout: 5s
      retries: 10
    restart: unless-stopped

  # --- MinIO (S3-compatible, optional) ---
  minio:
    image: minio/minio:latest
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # Console
    environment:
      MINIO_ROOT_USER: "minio_user"
      MINIO_ROOT_PASSWORD: "minio_password"
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -lc ':> /dev/tcp/127.0.0.1/9000' || exit 1"]
      interval: 2s
      timeout: 10s
      retries: 20
    profiles: ["dev"]
    restart: unless-stopped

  # Create bucket if missing (dev only)
  minio-create-bucket:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      bash -lc "
      mc alias set minio http://minio:9000 minio_user minio_password &&
      mc ls minio/mlops-fraud-dvc || mc mb minio/mlops-fraud-dvc
      "
    profiles: ["dev"]
    restart: "no"

  # --- MLflow Tracking Server ---
  mlflow:
    build:
      context: .
      dockerfile_inline: |
        FROM ghcr.io/mlflow/mlflow:v3.1.4
        RUN pip install --no-cache-dir psycopg2-binary==2.9.9 boto3==1.34.162
    container_name: mlflow-server
    depends_on:
      postgres:
        condition: service_healthy
      # For MinIO dev profile, also depend on these:
      # minio:
      #   condition: service_healthy
      # minio-create-bucket:
      #   condition: service_completed_successfully
    ports:
      - "5500:5000"   # host:container
    env_file: [.env]
    environment:
      # For AWS S3 (recommended): leave endpoint UNSET.
      # For MinIO dev: uncomment next line in your .env or here
      # MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
      AWS_REGION: ${AWS_REGION}
      ARTIFACTS_URI: ${ARTIFACTS_URI}        # e.g. s3://mlops-fraud-dvc  (no trailing slash)
    command: >
      mlflow server
        --host 0.0.0.0 --port 5000
        --backend-store-uri postgresql+psycopg2://user:password@postgres:5432/mlflowdb
        --artifacts-destination ${ARTIFACTS_URI}
        --serve-artifacts
    healthcheck:
      test: ["CMD-SHELL","bash -lc \":> /dev/tcp/127.0.0.1/5000\""]
      interval: 5s
      timeout: 3s
      retries: 30
    restart: unless-stopped

  # --- FastAPI Inference API ---
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    depends_on:
      mlflow:
        condition: service_healthy
    ports:
      - "8080:8080"
    env_file: [.env]
    environment:
      # Internal MLflow URL (not host port 5500)
      MLFLOW_TRACKING_URI: http://mlflow:5000
      MODEL_NAME: ${MODEL_NAME:-fraud_xgb}
      MODEL_ALIAS: ${MODEL_ALIAS:-staging}
      MODEL_REFRESH_SECS: ${MODEL_REFRESH_SECS:-120}
      ARTIFACTS_URI: ${ARTIFACTS_URI}        # used by direct-S3 loader in api.py
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
      AWS_REGION: ${AWS_REGION}
      SKIP_REGISTRY_RESOLVER: "1"          # always go direct to S3
      MLFLOW_HTTP_REQUEST_TIMEOUT: "5"     # keep small anyway
      MODEL_READY_TIMEOUT_SECS: "8"
    restart: unless-stopped
    # mount optional debug scripts (comment out if not needed)
    volumes:
      - ./scripts:/app/scripts:ro

  # --- Airflow --- (add this section)
  # See https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html
  # We'll use a simplified version without Celery for local runs.

  # Airflow DB (separate from MLflow's)
  airflow-db:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports: ["5433:5432"]  # Use host 5433 to avoid conflict
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Airflow services need a UID to avoid permission errors when writing logs.
  # Your local UID is used here. If you run `id -u` in your terminal, it should match.
  # On Docker Desktop for Mac/Windows, this is often not an issue, but it's good practice.
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    depends_on:
      airflow-db:
        condition: service_healthy
    user: "${AIRFLOW_UID:-50001}:0"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__DAGS_FOLDER: /opt/project/airflow/dags
    volumes:
      - .:/opt/project:rw  # Mount your whole project
    entrypoint: /bin/bash
    command: -c "airflow db init && airflow users create --username admin --password admin --firstname Anonymous --lastname User --role Admin --email admin@example.com"
    restart: on-failure

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      mlflow:
        condition: service_healthy
    user: "${AIRFLOW_UID:-50001}:0"
    ports:
      - "8888:8080"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__DAGS_FOLDER: /opt/project/airflow/dags
    volumes:
      - .:/opt/project:rw
    command: airflow webserver
    restart: unless-stopped

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      mlflow:
        condition: service_healthy
    user: "${AIRFLOW_UID:-50001}:0"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__DAGS_FOLDER: /opt/project/airflow/dags
    volumes:
      - .:/opt/project:rw
    command: airflow scheduler
    restart: unless-stopped

  streamlit:
    build:
      context: .
      dockerfile: Dockerfile.streamlit
    volumes:
      - ./streamlit:/app/streamlit:ro
    environment:
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI}
      API_BASE_URL: http://api:8080
    ports: ["8501:8501"]
    depends_on: ["api"]
